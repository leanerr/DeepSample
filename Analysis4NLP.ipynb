{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "616422bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data saved to DS4NLP_results/SUPS/imdb300AuxDS_confidence_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/imdb300AuxDS_dsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/imdb300AuxDS_entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/imdb300AuxDS_lsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/imdb300AuxDS_similarity_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/imdbAuxDS_confidence_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/imdbAuxDS_dsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/imdbAuxDS_entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/imdbAuxDS_lsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/imdbAuxDS_similarity_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/SSTIMDB3000AuxDS_confidence_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/SSTIMDB3000AuxDS_dsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/SSTIMDB3000AuxDS_entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/SSTIMDB3000AuxDS_lsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/SSTIMDB3000AuxDS_similarity_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/SSTtestAuxDS_confidence_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/SSTtestAuxDS_dsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/SSTtestAuxDS_entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/SSTtestAuxDS_lsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SUPS/SSTtestAuxDS_similarity_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/imdb300AuxDS_confidence_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/imdb300AuxDS_dsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/imdb300AuxDS_entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/imdb300AuxDS_lsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/imdb300AuxDS_similarity_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/imdbAuxDS_confidence_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/imdbAuxDS_dsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/imdbAuxDS_entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/imdbAuxDS_lsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/imdbAuxDS_similarity_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/SSTIMDB3000AuxDS_confidence_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/SSTIMDB3000AuxDS_dsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/SSTIMDB3000AuxDS_entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/SSTIMDB3000AuxDS_lsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/SSTIMDB3000AuxDS_similarity_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/SSTtestAuxDS_confidence_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/SSTtestAuxDS_dsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/SSTtestAuxDS_entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/SSTtestAuxDS_lsa_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/DeepEST/SSTtestAuxDS_similarity_agg.csv\n",
      "Data aggregation completed.\n"
     ]
    }
   ],
   "source": [
    "# Analysis4NLP.ipynb for SUPS and DeepEST\n",
    "\n",
    "# making aggregarion\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base directory and folder names\n",
    "base_dir = 'Results/Classification'\n",
    "folders = ['SUPS', 'DeepEST']\n",
    "datasets = ['imdb300AuxDS', 'imdbAuxDS', 'SSTIMDB3000AuxDS', 'SSTtestAuxDS']\n",
    "aux_vars = ['confidence', 'dsa', 'entropy', 'lsa', 'similarity']\n",
    "budgets = [50, 100, 200, 400, 800]\n",
    "\n",
    "# Define the output directory for aggregated results\n",
    "output_dir = 'DS4NLP_results'\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Function to read data based on file extension\n",
    "def read_data(file_path):\n",
    "    if file_path.endswith('.csv'):\n",
    "        return pd.read_csv(file_path)\n",
    "    elif file_path.endswith('.txt'):\n",
    "        return pd.read_csv(file_path, delimiter=',')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "\n",
    "# Iterate over each folder, dataset, and auxiliary variable\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    \n",
    "    # Create a subdirectory in the output directory for each folder\n",
    "    output_folder_path = os.path.join(output_dir, folder)\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for aux_var in aux_vars:\n",
    "            # Prepare a list to collect dataframes for aggregation\n",
    "            dataframes = []\n",
    "            for budget in budgets:\n",
    "                # Construct file names differently for DeepEST and others\n",
    "                if folder == 'DeepEST':\n",
    "                    file_name = f\"{dataset}.{aux_var}_{budget}.csv\"  # Use period for DeepEST\n",
    "                else:\n",
    "                    file_name = f\"{dataset}_{aux_var}_{budget}.txt\"  # Use underscore for others\n",
    "\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Check if the file exists\n",
    "                if os.path.exists(file_path):\n",
    "                    # Read the data and add a budget column\n",
    "                    try:\n",
    "                        data = read_data(file_path)\n",
    "                        data['budget'] = budget\n",
    "                        dataframes.append(data)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    print(f\"File not found: {file_name} in {folder}\")\n",
    "                    continue\n",
    "\n",
    "            # Concatenate all dataframes for this dataset and auxiliary variable\n",
    "            if dataframes:\n",
    "                aggregated_data = pd.concat(dataframes, ignore_index=True)\n",
    "                # Save the aggregated data to a new CSV file\n",
    "                output_file = f\"{dataset}_{aux_var}_agg.csv\"\n",
    "                output_path = os.path.join(output_folder_path, output_file)\n",
    "                try:\n",
    "                    aggregated_data.to_csv(output_path, index=False)\n",
    "                    print(f\"Aggregated data saved to {output_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving aggregated data for {dataset} - {aux_var}: {e}\")\n",
    "\n",
    "print(\"Data aggregation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "192c55bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data saved to DS4NLP_results/GBS/imdb300AuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/imdb300AuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/imdb300AuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/imdb300AuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/imdb300AuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/imdbAuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/imdbAuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/imdbAuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/imdbAuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/imdbAuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/SSTtestAuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/SSTtestAuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/SSTtestAuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/SSTtestAuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/GBS/SSTtestAuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/imdb300AuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/imdb300AuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/imdb300AuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/imdb300AuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/imdb300AuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/imdbAuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/imdbAuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/imdbAuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/imdbAuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/imdbAuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/SSTtestAuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/SSTtestAuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/SSTtestAuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/SSTtestAuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/RHC-S/SSTtestAuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/imdb300AuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/imdb300AuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/imdb300AuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/imdb300AuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/imdb300AuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/imdbAuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/imdbAuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/imdbAuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/imdbAuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/imdbAuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/SSTtestAuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/SSTtestAuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/SSTtestAuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/SSTtestAuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SSRS/SSTtestAuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/imdb300AuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/imdb300AuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/imdb300AuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/imdb300AuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/imdb300AuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/imdbAuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/imdbAuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/imdbAuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/imdbAuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/imdbAuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/SSTtestAuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/SSTtestAuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/SSTtestAuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/SSTtestAuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/SRS/SSTtestAuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/imdb300AuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/imdb300AuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/imdb300AuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/imdb300AuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/imdb300AuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/imdbAuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/imdbAuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/imdbAuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/imdbAuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/imdbAuxDS_Similarity_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/SSTtestAuxDS_Confidence_Score_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/SSTtestAuxDS_DSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/SSTtestAuxDS_LSA_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/SSTtestAuxDS_Prediction_Entropy_agg.csv\n",
      "Aggregated data saved to DS4NLP_results/2-UPS/SSTtestAuxDS_Similarity_Score_agg.csv\n",
      "Data aggregation completed.\n"
     ]
    }
   ],
   "source": [
    "# for 'GBS', 'RHC-S', 'SSRS'\n",
    "# making aggregarion\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base directory and folder names\n",
    "base_dir = 'Results/Classification'\n",
    "folders_to_process = ['GBS', 'RHC-S', 'SSRS','SRS', '2-UPS']\n",
    "\n",
    "# Define the datasets and auxiliary variables\n",
    "datasets = ['imdb300AuxDS', 'imdbAuxDS', 'SSTtestAuxDS']\n",
    "aux_vars = ['Confidence_Score', 'DSA', 'LSA', 'Prediction_Entropy', 'Similarity_Score']\n",
    "budgets = [50, 100, 200, 400, 800]\n",
    "\n",
    "# Define the output directory for aggregated results\n",
    "output_dir = 'DS4NLP_results'\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Function to read data based on file extension\n",
    "def read_data(file_path):\n",
    "    if file_path.endswith('.csv'):\n",
    "        return pd.read_csv(file_path)\n",
    "    elif file_path.endswith('.txt'):\n",
    "        return pd.read_csv(file_path, delimiter=',')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "\n",
    "# Iterate over each specified folder\n",
    "for folder in folders_to_process:\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    \n",
    "    # Create a subdirectory in the output directory for each folder\n",
    "    output_folder_path = os.path.join(output_dir, folder)\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for aux_var in aux_vars:\n",
    "            # Prepare a list to collect dataframes for aggregation\n",
    "            dataframes = []\n",
    "            for budget in budgets:\n",
    "                # Construct file name for txt files\n",
    "                file_name = f\"{dataset}_{aux_var}_{budget}.txt\"\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Check if the file exists\n",
    "                if os.path.exists(file_path):\n",
    "                    # Read the data and add a budget column\n",
    "                    try:\n",
    "                        data = read_data(file_path)\n",
    "                        data['budget'] = budget\n",
    "                        dataframes.append(data)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    print(f\"File not found: {file_name} in {folder}\")\n",
    "                    continue\n",
    "\n",
    "            # Concatenate all dataframes for this dataset and auxiliary variable\n",
    "            if dataframes:\n",
    "                aggregated_data = pd.concat(dataframes, ignore_index=True)\n",
    "                # Save the aggregated data to a new CSV file\n",
    "                output_file = f\"{dataset}_{aux_var}_agg.csv\"\n",
    "                output_path = os.path.join(output_folder_path, output_file)\n",
    "                try:\n",
    "                    aggregated_data.to_csv(output_path, index=False)\n",
    "                    print(f\"Aggregated data saved to {output_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving aggregated data for {dataset} - {aux_var}: {e}\")\n",
    "\n",
    "print(\"Data aggregation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a6179",
   "metadata": {},
   "source": [
    "## RQ1,2 : RMSE, RMedSE, Mean Failure, STD Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93c007ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      method       dataset           aux_var      RMSE    RMedSE  \\\n",
      "0    DeepEST     imdbAuxDS               LSA  0.006781  0.003408   \n",
      "1    DeepEST  imdb300AuxDS  Confidence_Score  0.049166  0.023258   \n",
      "2    DeepEST  imdb300AuxDS  Similarity_Score  0.057486  0.023240   \n",
      "3    DeepEST  imdb300AuxDS               LSA  0.007841  0.002724   \n",
      "4    DeepEST  SSTtestAuxDS  Confidence_Score  0.041553  0.015825   \n",
      "..       ...           ...               ...       ...       ...   \n",
      "100    2-UPS     imdbAuxDS  Confidence_Score  0.028804  0.015395   \n",
      "101    2-UPS  SSTtestAuxDS  Similarity_Score  0.024862  0.012570   \n",
      "102    2-UPS  imdb300AuxDS               DSA  0.042787  0.029765   \n",
      "103    2-UPS  SSTtestAuxDS  Confidence_Score  0.028390  0.013943   \n",
      "104    2-UPS     imdbAuxDS               DSA  0.048547  0.029793   \n",
      "\n",
      "     Mean Failures  STD Failures  \n",
      "0       254.460000    224.883493  \n",
      "1        67.366667     38.147087  \n",
      "2         7.040000      6.780232  \n",
      "3       175.446667    106.900602  \n",
      "4        42.060000     18.766753  \n",
      "..             ...           ...  \n",
      "100      34.466667     31.643498  \n",
      "101      23.946667     21.695819  \n",
      "102      38.073333     35.706878  \n",
      "103      26.173333     23.922343  \n",
      "104      42.446667     38.590874  \n",
      "\n",
      "[105 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the base path and methods\n",
    "base_path = \"DS4NLP_results\"\n",
    "methods = [\"DeepEST\", \"GBS\", \"RHC-S\", \"SSRS\", \"SUPS\", 'SRS', '2-UPS']\n",
    "\n",
    "# True accuracies for each dataset\n",
    "true_accuracies = {\n",
    "    \"imdb300AuxDS\": 0.8990,\n",
    "    \"imdbAuxDS\": 0.8896,\n",
    "    \"SSTtestAuxDS\": 0.9225700164744646\n",
    "}\n",
    "\n",
    "# Auxiliary variable mapping\n",
    "aux_var_mapping = {\n",
    "    \"confidence\": \"Confidence_Score\",\n",
    "    \"Confidence_Score\": \"Confidence_Score\",\n",
    "    \"entropy\": \"Prediction_Entropy\",\n",
    "    \"prediction\": \"Prediction_Entropy\",  \n",
    "    \"Prediction_Entropy\": \"Prediction_Entropy\",\n",
    "    \"similarity\": \"Similarity_Score\",\n",
    "    \"Similarity_Score\": \"Similarity_Score\",\n",
    "    \"dsa\": \"DSA\",\n",
    "    \"DSA\": \"DSA\",\n",
    "    \"lsa\": \"LSA\",\n",
    "    \"LSA\": \"LSA\"\n",
    "}\n",
    "\n",
    "# Function to calculate RMSE and RMedSE\n",
    "def calculate_rmse_rmedse(accuracies, true_accuracy):\n",
    "    squared_errors = (accuracies - true_accuracy) ** 2\n",
    "    rmse = np.sqrt(np.mean(squared_errors))\n",
    "    rmedse = np.sqrt(np.median(squared_errors))\n",
    "    return rmse, rmedse\n",
    "\n",
    "results = []\n",
    "\n",
    "# Process each method and its associated files\n",
    "for method in methods:\n",
    "    method_path = os.path.join(base_path, method)\n",
    "    if not os.path.exists(method_path):\n",
    "        print(f\"Directory not found: {method_path}\")\n",
    "        continue\n",
    "\n",
    "    # List all files in the method directory\n",
    "    for file_name in os.listdir(method_path):\n",
    "        if not file_name.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        dataset = file_name.split('_')[0]\n",
    "        \n",
    "        if dataset not in true_accuracies:\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(method_path, file_name)\n",
    "        \n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "            if data.empty:\n",
    "                print(f\"No data in {file_name}\")\n",
    "                continue\n",
    "\n",
    "            # Processing accuracies\n",
    "            accuracies = data['accuracy'].values\n",
    "            rmse, rmedse = calculate_rmse_rmedse(accuracies, true_accuracies[dataset])\n",
    "            \n",
    "            # Processing failures\n",
    "            mean_failures = data['failures'].mean()\n",
    "            std_failures = data['failures'].std()\n",
    "            \n",
    "            # Map auxiliary variable name\n",
    "            aux_var_key = file_name.split('_')[1].split('.')[0].lower()\n",
    "            aux_var = aux_var_mapping.get(aux_var_key, \"Unknown Variable\")\n",
    "\n",
    "            results.append({\n",
    "                \"method\": method,\n",
    "                \"dataset\": dataset,\n",
    "                \"aux_var\": aux_var,\n",
    "                \"RMSE\": rmse,\n",
    "                \"RMedSE\": rmedse,\n",
    "                \"Mean Failures\": mean_failures,\n",
    "                \"STD Failures\": std_failures\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\"rmse_rmedse_failures.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ddf75189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>dataset</th>\n",
       "      <th>aux_var</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>RMedSE</th>\n",
       "      <th>Mean Failures</th>\n",
       "      <th>STD Failures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeepEST</td>\n",
       "      <td>imdbAuxDS</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>254.460000</td>\n",
       "      <td>224.883493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DeepEST</td>\n",
       "      <td>imdb300AuxDS</td>\n",
       "      <td>Confidence_Score</td>\n",
       "      <td>0.049166</td>\n",
       "      <td>0.023258</td>\n",
       "      <td>67.366667</td>\n",
       "      <td>38.147087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepEST</td>\n",
       "      <td>imdb300AuxDS</td>\n",
       "      <td>Similarity_Score</td>\n",
       "      <td>0.057486</td>\n",
       "      <td>0.023240</td>\n",
       "      <td>7.040000</td>\n",
       "      <td>6.780232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepEST</td>\n",
       "      <td>imdb300AuxDS</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.007841</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>175.446667</td>\n",
       "      <td>106.900602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeepEST</td>\n",
       "      <td>SSTtestAuxDS</td>\n",
       "      <td>Confidence_Score</td>\n",
       "      <td>0.041553</td>\n",
       "      <td>0.015825</td>\n",
       "      <td>42.060000</td>\n",
       "      <td>18.766753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2-UPS</td>\n",
       "      <td>imdbAuxDS</td>\n",
       "      <td>Confidence_Score</td>\n",
       "      <td>0.028804</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>34.466667</td>\n",
       "      <td>31.643498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2-UPS</td>\n",
       "      <td>SSTtestAuxDS</td>\n",
       "      <td>Similarity_Score</td>\n",
       "      <td>0.024862</td>\n",
       "      <td>0.012570</td>\n",
       "      <td>23.946667</td>\n",
       "      <td>21.695819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2-UPS</td>\n",
       "      <td>imdb300AuxDS</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.042787</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>38.073333</td>\n",
       "      <td>35.706878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2-UPS</td>\n",
       "      <td>SSTtestAuxDS</td>\n",
       "      <td>Confidence_Score</td>\n",
       "      <td>0.028390</td>\n",
       "      <td>0.013943</td>\n",
       "      <td>26.173333</td>\n",
       "      <td>23.922343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2-UPS</td>\n",
       "      <td>imdbAuxDS</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.048547</td>\n",
       "      <td>0.029793</td>\n",
       "      <td>42.446667</td>\n",
       "      <td>38.590874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      method       dataset           aux_var      RMSE    RMedSE  \\\n",
       "0    DeepEST     imdbAuxDS               LSA  0.006781  0.003408   \n",
       "1    DeepEST  imdb300AuxDS  Confidence_Score  0.049166  0.023258   \n",
       "2    DeepEST  imdb300AuxDS  Similarity_Score  0.057486  0.023240   \n",
       "3    DeepEST  imdb300AuxDS               LSA  0.007841  0.002724   \n",
       "4    DeepEST  SSTtestAuxDS  Confidence_Score  0.041553  0.015825   \n",
       "..       ...           ...               ...       ...       ...   \n",
       "100    2-UPS     imdbAuxDS  Confidence_Score  0.028804  0.015395   \n",
       "101    2-UPS  SSTtestAuxDS  Similarity_Score  0.024862  0.012570   \n",
       "102    2-UPS  imdb300AuxDS               DSA  0.042787  0.029765   \n",
       "103    2-UPS  SSTtestAuxDS  Confidence_Score  0.028390  0.013943   \n",
       "104    2-UPS     imdbAuxDS               DSA  0.048547  0.029793   \n",
       "\n",
       "     Mean Failures  STD Failures  \n",
       "0       254.460000    224.883493  \n",
       "1        67.366667     38.147087  \n",
       "2         7.040000      6.780232  \n",
       "3       175.446667    106.900602  \n",
       "4        42.060000     18.766753  \n",
       "..             ...           ...  \n",
       "100      34.466667     31.643498  \n",
       "101      23.946667     21.695819  \n",
       "102      38.073333     35.706878  \n",
       "103      26.173333     23.922343  \n",
       "104      42.446667     38.590874  \n",
       "\n",
       "[105 rows x 7 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "922cd3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"DeepEST\": [\n",
    "        \"imdb300AuxDS_confidence_agg.csv\",\n",
    "        \"imdb300AuxDS_dsa_agg.csv\",\n",
    "        \"imdb300AuxDS_entropy_agg.csv\",\n",
    "        \"imdb300AuxDS_lsa_agg.csv\",\n",
    "        \"imdb300AuxDS_similarity_agg.csv\",\n",
    "        \"imdbAuxDS_confidence_agg.csv\",\n",
    "        \"imdbAuxDS_dsa_agg.csv\",\n",
    "        \"imdbAuxDS_entropy_agg.csv\",\n",
    "        \"imdbAuxDS_lsa_agg.csv\",\n",
    "        \"imdbAuxDS_similarity_agg.csv\",\n",
    "        \"SSTtestAuxDS_confidence_agg.csv\",\n",
    "        \"SSTtestAuxDS_dsa_agg.csv\",\n",
    "        \"SSTtestAuxDS_entropy_agg.csv\",\n",
    "        \"SSTtestAuxDS_lsa_agg.csv\",\n",
    "        \"SSTtestAuxDS_similarity_agg.csv\"\n",
    "    ],\n",
    "    \n",
    "    \"GBS\" : [\n",
    "        \"imdb300AuxDS_Confidence_Score_agg.csv\",\n",
    "        \"imdb300AuxDS_dsa_agg.csv\",\n",
    "        \"imdb300AuxDS_lsa_agg.csv\",\n",
    "        \"imdb300AuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"imdb300AuxDS_Similarity_Score_agg.csv\",\n",
    "        \"imdbAuxDS_Confidence_Score_agg.csv\",\n",
    "        \"imdbAuxDS_dsa_agg.csv\",\n",
    "        \"imdbAuxDS_lsa_agg.csv\",\n",
    "        \"imdbAuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"imdbAuxDS_Similarity_Score_agg.csv\",\n",
    "        \"SSTtestAuxDS_Confidence_Score_agg.csv\",\n",
    "        \"SSTtestAuxDS_dsa_agg.csv\",\n",
    "        \"SSTtestAuxDS_lsa_agg.csv\",\n",
    "        \"SSTtestAuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"SSTtestAuxDS_Similarity_Score_agg.csv\"\n",
    "         ]\n",
    "    ,\n",
    "    \"RHC-S\": [\n",
    "        \"imdb300AuxDS_Confidence_Score_agg.csv\",\n",
    "        \"imdb300AuxDS_dsa_agg.csv\",\n",
    "        \"imdb300AuxDS_lsa_agg.csv\",\n",
    "        \"imdb300AuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"imdb300AuxDS_Similarity_Score_agg.csv\",\n",
    "        \"imdbAuxDS_Confidence_Score_agg.csv\",\n",
    "        \"imdbAuxDS_dsa_agg.csv\",\n",
    "        \"imdbAuxDS_lsa_agg.csv\",\n",
    "        \"imdbAuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"imdbAuxDS_Similarity_Score_agg.csv\",\n",
    "        \"SSTtestAuxDS_Confidence_Score_agg.csv\",\n",
    "        \"SSTtestAuxDS_dsa_agg.csv\",\n",
    "        \"SSTtestAuxDS_lsa_agg.csv\",\n",
    "        \"SSTtestAuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"SSTtestAuxDS_Similarity_Score_agg.csv\"\n",
    "    ],\n",
    "    \"SSRS\": [\n",
    "        \"imdb300AuxDS_Confidence_Score_agg.csv\",\n",
    "        \"imdb300AuxDS_dsa_agg.csv\",\n",
    "        \"imdb300AuxDS_lsa_agg.csv\",\n",
    "        \"imdb300AuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"imdb300AuxDS_Similarity_Score_agg.csv\",\n",
    "        \"imdbAuxDS_Confidence_Score_agg.csv\",\n",
    "        \"imdbAuxDS_dsa_agg.csv\",\n",
    "        \"imdbAuxDS_lsa_agg.csv\",\n",
    "        \"imdbAuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"imdbAuxDS_Similarity_Score_agg.csv\",\n",
    "        \"SSTtestAuxDS_Confidence_Score_agg.csv\",\n",
    "        \"SSTtestAuxDS_dsa_agg.csv\",\n",
    "        \"SSTtestAuxDS_lsa_agg.csv\",\n",
    "        \"SSTtestAuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"SSTtestAuxDS_Similarity_Score_agg.csv\"\n",
    "    ],\n",
    "    \"SUPS\": [\n",
    "        \"imdb300AuxDS_confidence_agg.csv\",\n",
    "        \"imdb300AuxDS_dsa_agg.csv\",\n",
    "        \"imdb300AuxDS_entropy_agg.csv\",\n",
    "        \"imdb300AuxDS_lsa_agg.csv\",\n",
    "        \"imdb300AuxDS_similarity_agg.csv\",\n",
    "        \"imdbAuxDS_confidence_agg.csv\",\n",
    "        \"imdbAuxDS_dsa_agg.csv\",\n",
    "        \"imdbAuxDS_entropy_agg.csv\",\n",
    "        \"imdbAuxDS_lsa_agg.csv\",\n",
    "        \"imdbAuxDS_similarity_agg.csv\",\n",
    "        \"SSTtestAuxDS_confidence_agg.csv\",\n",
    "        \"SSTtestAuxDS_dsa_agg.csv\",\n",
    "        \"SSTtestAuxDS_entropy_agg.csv\",\n",
    "        \"SSTtestAuxDS_lsa_agg.csv\",\n",
    "        \"SSTtestAuxDS_similarity_agg.csv\"\n",
    "    ]\n",
    "    ,\n",
    "    \"SRS\" : [\n",
    "        \"imdb300AuxDS_Confidence_Score_agg.csv\",\n",
    "        \"imdb300AuxDS_DSA_agg.csv\",\n",
    "        \"imdb300AuxDS_LSA_agg.csv\",\n",
    "        \"imdb300AuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"imdb300AuxDS_Similarity_Score_agg.csv\",\n",
    "        \"imdbAuxDS_Confidence_Score_agg.csv\",\n",
    "        \"imdbAuxDS_DSA_agg.csv\",\n",
    "        \"imdbAuxDS_LSA_agg.csv\",\n",
    "        \"imdbAuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"imdbAuxDS_Similarity_Score_agg.csv\",\n",
    "        \"SSTtestAuxDS_Confidence_Score_agg.csv\",\n",
    "        \"SSTtestAuxDS_DSA_agg.csv\",\n",
    "        \"SSTtestAuxDS_LSA_agg.csv\",\n",
    "        \"SSTtestAuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"SSTtestAuxDS_Similarity_Score_agg.csv\"\n",
    "    ]\n",
    "    , \n",
    "    \"2-UPS\" : [\n",
    "        \"imdb300AuxDS_Confidence_Score_agg.csv\",\n",
    "        \"imdb300AuxDS_DSA_agg.csv\",\n",
    "        \"imdb300AuxDS_LSA_agg.csv\",\n",
    "        \"imdb300AuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"imdb300AuxDS_Similarity_Score_agg.csv\",\n",
    "        \"imdbAuxDS_Confidence_Score_agg.csv\",\n",
    "        \"imdbAuxDS_DSA_agg.csv\",\n",
    "        \"imdbAuxDS_LSA_agg.csv\",\n",
    "        \"imdbAuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"imdbAuxDS_Similarity_Score_agg.csv\",\n",
    "        \"SSTtestAuxDS_Confidence_Score_agg.csv\",\n",
    "        \"SSTtestAuxDS_DSA_agg.csv\",\n",
    "        \"SSTtestAuxDS_LSA_agg.csv\",\n",
    "        \"SSTtestAuxDS_Prediction_Entropy_agg.csv\",\n",
    "        \"SSTtestAuxDS_Similarity_Score_agg.csv\"\n",
    "         ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1edfa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: DS4NLP_results/DeepEST/imdb300AuxDS_confidence_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/imdb300AuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/imdb300AuxDS_entropy_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/imdb300AuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/imdb300AuxDS_similarity_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/imdbAuxDS_confidence_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/imdbAuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/imdbAuxDS_entropy_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/imdbAuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/imdbAuxDS_similarity_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/SSTtestAuxDS_confidence_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/SSTtestAuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/SSTtestAuxDS_entropy_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/SSTtestAuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/DeepEST/SSTtestAuxDS_similarity_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/imdb300AuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/imdb300AuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/imdb300AuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/imdb300AuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/imdb300AuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/imdbAuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/imdbAuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/imdbAuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/imdbAuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/imdbAuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/SSTtestAuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/SSTtestAuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/SSTtestAuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/SSTtestAuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/GBS/SSTtestAuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/imdb300AuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/imdb300AuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/imdb300AuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/imdb300AuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/imdb300AuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/imdbAuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/imdbAuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/imdbAuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/imdbAuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/imdbAuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/SSTtestAuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/SSTtestAuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/SSTtestAuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/SSTtestAuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/RHC-S/SSTtestAuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/imdb300AuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/imdb300AuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/imdb300AuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/imdb300AuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/imdb300AuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/imdbAuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/imdbAuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/imdbAuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/imdbAuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/imdbAuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/SSTtestAuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/SSTtestAuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/SSTtestAuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/SSTtestAuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/SSRS/SSTtestAuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/imdb300AuxDS_confidence_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/imdb300AuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/imdb300AuxDS_entropy_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/imdb300AuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/imdb300AuxDS_similarity_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/imdbAuxDS_confidence_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/imdbAuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/imdbAuxDS_entropy_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/imdbAuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/imdbAuxDS_similarity_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/SSTtestAuxDS_confidence_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/SSTtestAuxDS_dsa_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/SSTtestAuxDS_entropy_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/SSTtestAuxDS_lsa_agg.csv\n",
      "Processing file: DS4NLP_results/SUPS/SSTtestAuxDS_similarity_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/imdb300AuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/imdb300AuxDS_DSA_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/imdb300AuxDS_LSA_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/imdb300AuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/imdb300AuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/imdbAuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/imdbAuxDS_DSA_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/imdbAuxDS_LSA_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/imdbAuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/imdbAuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/SSTtestAuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/SSTtestAuxDS_DSA_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/SSTtestAuxDS_LSA_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/SSTtestAuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/SRS/SSTtestAuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/imdb300AuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/imdb300AuxDS_DSA_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/imdb300AuxDS_LSA_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/imdb300AuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/imdb300AuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/imdbAuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/imdbAuxDS_DSA_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/imdbAuxDS_LSA_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/imdbAuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/imdbAuxDS_Similarity_Score_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/SSTtestAuxDS_Confidence_Score_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/SSTtestAuxDS_DSA_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/SSTtestAuxDS_LSA_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/SSTtestAuxDS_Prediction_Entropy_agg.csv\n",
      "Processing file: DS4NLP_results/2-UPS/SSTtestAuxDS_Similarity_Score_agg.csv\n",
      "      method       dataset             aux_var      RMSE    RMedSE  \\\n",
      "0    DeepEST  imdb300AuxDS    Confidence_Score  0.046567  0.026221   \n",
      "1    DeepEST  imdb300AuxDS                 DSA  0.016135  0.008057   \n",
      "2    DeepEST  imdb300AuxDS  Prediction_Entropy  0.041714  0.026660   \n",
      "3    DeepEST  imdb300AuxDS                 LSA  0.007369  0.002580   \n",
      "4    DeepEST  imdb300AuxDS    Similarity_Score  0.033537  0.019886   \n",
      "..       ...           ...                 ...       ...       ...   \n",
      "100    2-UPS  SSTtestAuxDS    Confidence_Score  0.017719  0.012720   \n",
      "101    2-UPS  SSTtestAuxDS                 DSA  0.058226  0.052578   \n",
      "102    2-UPS  SSTtestAuxDS                 LSA  0.072877  0.072430   \n",
      "103    2-UPS  SSTtestAuxDS  Prediction_Entropy  0.060796  0.059982   \n",
      "104    2-UPS  SSTtestAuxDS    Similarity_Score  0.016463  0.007570   \n",
      "\n",
      "     failures_mean  failures_std  \n",
      "0        72.233333      2.967448  \n",
      "1        20.300000      3.281610  \n",
      "2        73.300000      3.621297  \n",
      "3       161.533333      5.975429  \n",
      "4         4.333333      1.422318  \n",
      "..             ...           ...  \n",
      "100      16.566667      3.297683  \n",
      "101      24.600000      6.703576  \n",
      "102       0.933333      0.827682  \n",
      "103       3.466667      1.870521  \n",
      "104      15.200000      3.336321  \n",
      "\n",
      "[105 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the base path and methods\n",
    "base_path = \"DS4NLP_results\"\n",
    "methods = [\"DeepEST\", \"GBS\", \"RHC-S\", \"SSRS\", \"SUPS\", 'SRS', '2-UPS']\n",
    "\n",
    "# True accuracies for each dataset\n",
    "true_accuracies = {\n",
    "    \"imdb300AuxDS\": 0.8990,\n",
    "    \"imdbAuxDS\": 0.8896,\n",
    "    \"SSTtestAuxDS\": 0.9225700164744646\n",
    "}\n",
    "\n",
    "# Auxiliary variable mapping\n",
    "aux_var_mapping = {\n",
    "    \"confidence\": \"Confidence_Score\",\n",
    "    \"Confidence_Score\": \"Confidence_Score\",\n",
    "    \"entropy\": \"Prediction_Entropy\",\n",
    "    \"prediction\": \"Prediction_Entropy\",  \n",
    "    \"Prediction_Entropy\": \"Prediction_Entropy\",\n",
    "    \"similarity\": \"Similarity_Score\",\n",
    "    \"Similarity_Score\": \"Similarity_Score\",\n",
    "    \"dsa\": \"DSA\",\n",
    "    \"DSA\": \"DSA\",\n",
    "    \"lsa\": \"LSA\",\n",
    "    \"LSA\": \"LSA\"\n",
    "}\n",
    "\n",
    "# Function to calculate RMSE and RMedSE\n",
    "def calculate_rmse_rmedse(accuracies, true_accuracy):\n",
    "    squared_errors = (accuracies - true_accuracy) ** 2\n",
    "    rmse = np.sqrt(np.mean(squared_errors))\n",
    "    rmedse = np.sqrt(np.median(squared_errors))\n",
    "    return rmse, rmedse\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over each method and dataset\n",
    "for method in methods:\n",
    "    for file_name in datasets[method]:\n",
    "        dataset = file_name.split('_')[0]\n",
    "        \n",
    "        if dataset not in true_accuracies:\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(base_path, method, file_name)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Read the data\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "            data = data[data['budget'] == 200]  # Focus on budget 200\n",
    "            if data.empty:\n",
    "                print(f\"No records with budget 200 in {file_name}\")\n",
    "                continue\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            accuracies = data['accuracy'].values\n",
    "            rmse, rmedse = calculate_rmse_rmedse(accuracies, true_accuracies[dataset])\n",
    "            failures_mean = data['failures'].mean()\n",
    "            failures_std = data['failures'].std()\n",
    "            \n",
    "            results.append({\n",
    "                \"method\": method,\n",
    "                \"dataset\": dataset,\n",
    "                \"aux_var\": aux_var_mapping.get(file_name.split('_')[1].split('.')[0].lower(), \"Unknown Variable\"),\n",
    "                \"RMSE\": rmse,\n",
    "                \"RMedSE\": rmedse,\n",
    "                \"failures_mean\": failures_mean,\n",
    "                \"failures_std\": failures_std\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file for records with budget 200\n",
    "results_df.to_csv(\"rmse_rmedse_failures200.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "91a9595a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>dataset</th>\n",
       "      <th>aux_var</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>RMedSE</th>\n",
       "      <th>failures_mean</th>\n",
       "      <th>failures_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeepEST</td>\n",
       "      <td>imdb300AuxDS</td>\n",
       "      <td>Confidence_Score</td>\n",
       "      <td>0.046567</td>\n",
       "      <td>0.026221</td>\n",
       "      <td>72.233333</td>\n",
       "      <td>2.967448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DeepEST</td>\n",
       "      <td>imdb300AuxDS</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.016135</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>20.300000</td>\n",
       "      <td>3.281610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepEST</td>\n",
       "      <td>imdb300AuxDS</td>\n",
       "      <td>Prediction_Entropy</td>\n",
       "      <td>0.041714</td>\n",
       "      <td>0.026660</td>\n",
       "      <td>73.300000</td>\n",
       "      <td>3.621297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepEST</td>\n",
       "      <td>imdb300AuxDS</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>161.533333</td>\n",
       "      <td>5.975429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeepEST</td>\n",
       "      <td>imdb300AuxDS</td>\n",
       "      <td>Similarity_Score</td>\n",
       "      <td>0.033537</td>\n",
       "      <td>0.019886</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>1.422318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2-UPS</td>\n",
       "      <td>SSTtestAuxDS</td>\n",
       "      <td>Confidence_Score</td>\n",
       "      <td>0.017719</td>\n",
       "      <td>0.012720</td>\n",
       "      <td>16.566667</td>\n",
       "      <td>3.297683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2-UPS</td>\n",
       "      <td>SSTtestAuxDS</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.058226</td>\n",
       "      <td>0.052578</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>6.703576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2-UPS</td>\n",
       "      <td>SSTtestAuxDS</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.072877</td>\n",
       "      <td>0.072430</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.827682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2-UPS</td>\n",
       "      <td>SSTtestAuxDS</td>\n",
       "      <td>Prediction_Entropy</td>\n",
       "      <td>0.060796</td>\n",
       "      <td>0.059982</td>\n",
       "      <td>3.466667</td>\n",
       "      <td>1.870521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2-UPS</td>\n",
       "      <td>SSTtestAuxDS</td>\n",
       "      <td>Similarity_Score</td>\n",
       "      <td>0.016463</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>3.336321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      method       dataset             aux_var      RMSE    RMedSE  \\\n",
       "0    DeepEST  imdb300AuxDS    Confidence_Score  0.046567  0.026221   \n",
       "1    DeepEST  imdb300AuxDS                 DSA  0.016135  0.008057   \n",
       "2    DeepEST  imdb300AuxDS  Prediction_Entropy  0.041714  0.026660   \n",
       "3    DeepEST  imdb300AuxDS                 LSA  0.007369  0.002580   \n",
       "4    DeepEST  imdb300AuxDS    Similarity_Score  0.033537  0.019886   \n",
       "..       ...           ...                 ...       ...       ...   \n",
       "100    2-UPS  SSTtestAuxDS    Confidence_Score  0.017719  0.012720   \n",
       "101    2-UPS  SSTtestAuxDS                 DSA  0.058226  0.052578   \n",
       "102    2-UPS  SSTtestAuxDS                 LSA  0.072877  0.072430   \n",
       "103    2-UPS  SSTtestAuxDS  Prediction_Entropy  0.060796  0.059982   \n",
       "104    2-UPS  SSTtestAuxDS    Similarity_Score  0.016463  0.007570   \n",
       "\n",
       "     failures_mean  failures_std  \n",
       "0        72.233333      2.967448  \n",
       "1        20.300000      3.281610  \n",
       "2        73.300000      3.621297  \n",
       "3       161.533333      5.975429  \n",
       "4         4.333333      1.422318  \n",
       "..             ...           ...  \n",
       "100      16.566667      3.297683  \n",
       "101      24.600000      6.703576  \n",
       "102       0.933333      0.827682  \n",
       "103       3.466667      1.870521  \n",
       "104      15.200000      3.336321  \n",
       "\n",
       "[105 rows x 7 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5cac0435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 15 datasets for method: DeepEST\n",
      "Processing 15 datasets for method: GBS\n",
      "Processing 15 datasets for method: RHC-S\n",
      "Processing 15 datasets for method: SSRS\n",
      "Processing 15 datasets for method: SUPS\n",
      "Processing 15 datasets for method: SRS\n",
      "Processing 15 datasets for method: 2-UPS\n",
      "Friedman test statistic: 22.415770609319, p-value: 0.0010177298285168515\n",
      "Dunn's posthoc test results:\n",
      "            2-UPS   DeepEST       GBS    RHC-S       SRS      SSRS      SUPS\n",
      "2-UPS    1.000000  1.000000  1.000000  1.00000  0.387895  0.500097  1.000000\n",
      "DeepEST  1.000000  1.000000  1.000000  1.00000  1.000000  0.009094  1.000000\n",
      "GBS      1.000000  1.000000  1.000000  1.00000  1.000000  0.013886  1.000000\n",
      "RHC-S    1.000000  1.000000  1.000000  1.00000  1.000000  0.115680  1.000000\n",
      "SRS      0.387895  1.000000  1.000000  1.00000  1.000000  0.000082  0.834856\n",
      "SSRS     0.500097  0.009094  0.013886  0.11568  0.000082  1.000000  0.219897\n",
      "SUPS     1.000000  1.000000  1.000000  1.00000  0.834856  0.219897  1.000000\n"
     ]
    }
   ],
   "source": [
    "#budget 200, fridman dunn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Define the base path and methods\n",
    "base_path = \"DS4NLP_results\"\n",
    "methods = [\"DeepEST\", \"GBS\", \"RHC-S\", \"SSRS\", \"SUPS\", 'SRS', '2-UPS']\n",
    "\n",
    "\n",
    "\n",
    "# Auxiliary variable mapping\n",
    "aux_var_mapping = {\n",
    "    \"confidence\": \"Confidence_Score\",\n",
    "    \"Confidence_Score\": \"Confidence_Score\",\n",
    "    \"entropy\": \"Prediction_Entropy\",\n",
    "    \"prediction\": \"Prediction_Entropy\",\n",
    "    \"Prediction_Entropy\": \"Prediction_Entropy\",\n",
    "    \"similarity\": \"Similarity_Score\",\n",
    "    \"Similarity_Score\": \"Similarity_Score\",\n",
    "    \"dsa\": \"DSA\",\n",
    "    \"DSA\": \"DSA\",\n",
    "    \"lsa\": \"LSA\",\n",
    "    \"LSA\": \"LSA\"\n",
    "}\n",
    "\n",
    "# True accuracies for each dataset\n",
    "true_accuracies = {\n",
    "    \"imdb300AuxDS\": 0.8990,\n",
    "    \"imdbAuxDS\": 0.8896,\n",
    "    \"SSTtestAuxDS\": 0.9226\n",
    "}\n",
    "\n",
    "def calculate_rmse_rmedse(accuracies, true_accuracy):\n",
    "    squared_errors = (accuracies - true_accuracy) ** 2\n",
    "    rmse = np.sqrt(np.mean(squared_errors))\n",
    "    rmedse = np.sqrt(np.median(squared_errors))\n",
    "    return rmse, rmedse\n",
    "\n",
    "results = []\n",
    "\n",
    "# Processing each dataset for each method\n",
    "for method in methods:\n",
    "    dataset_count = len(datasets[method])\n",
    "    print(f\"Processing {dataset_count} datasets for method: {method}\")\n",
    "    for file_name in datasets[method]:\n",
    "        dataset = file_name.split('_')[0]\n",
    "        true_accuracy = true_accuracies.get(dataset, None)\n",
    "        if not true_accuracy:\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(base_path, method, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "            data = data[data['budget'] == 200]  # Filter for budget 200\n",
    "            if data.empty:\n",
    "                continue\n",
    "            accuracies = data['accuracy'].apply(lambda x: float(str(x).split(\":\")[-1])).values\n",
    "            aux_var_key = file_name.split('_')[1].replace(\".csv\", \"\").lower()\n",
    "            aux_var = aux_var_mapping.get(aux_var_key, \"Unknown Variable\")\n",
    "            rmse, rmedse = calculate_rmse_rmedse(accuracies, true_accuracy)\n",
    "\n",
    "            results.append({\n",
    "                \"method\": method,\n",
    "                \"dataset\": dataset,\n",
    "                \"aux_var\": aux_var,\n",
    "                \"RMSE\": rmse,\n",
    "                \"RMedSE\": rmedse\n",
    "            })\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "pivot_df = results_df.pivot_table(index=['dataset', 'aux_var'], columns='method', values='RMSE', aggfunc=np.mean)\n",
    "\n",
    "# Perform the Friedman test\n",
    "stat, p_value = friedmanchisquare(*[pivot_df[method].dropna().values for method in methods])\n",
    "print(f'Friedman test statistic: {stat}, p-value: {p_value}')\n",
    "\n",
    "# If the Friedman test is significant, proceed with Dunn's test\n",
    "if p_value < 0.05:\n",
    "    melted_df = pivot_df.reset_index().melt(id_vars=['dataset', 'aux_var'], var_name='method', value_name='RMSE')\n",
    "    dunn_results = sp.posthoc_dunn(melted_df, val_col='RMSE', group_col='method', p_adjust='bonferroni')\n",
    "    print(\"Dunn's posthoc test results:\")\n",
    "    print(dunn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c9e75803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman test statistic: 21.96414852752881, p-value: 0.0012291198830390786\n",
      "Dunn's posthoc test results:\n",
      "            2-UPS  DeepEST       GBS     RHC-S       SRS      SSRS      SUPS\n",
      "2-UPS    1.000000  1.00000  1.000000  1.000000  0.320634  0.384304  1.000000\n",
      "DeepEST  1.000000  1.00000  1.000000  1.000000  1.000000  0.031250  1.000000\n",
      "GBS      1.000000  1.00000  1.000000  1.000000  1.000000  0.025383  1.000000\n",
      "RHC-S    1.000000  1.00000  1.000000  1.000000  0.233496  0.518115  1.000000\n",
      "SRS      0.320634  1.00000  1.000000  0.233496  1.000000  0.000036  1.000000\n",
      "SSRS     0.384304  0.03125  0.025383  0.518115  0.000036  1.000000  0.091429\n",
      "SUPS     1.000000  1.00000  1.000000  1.000000  1.000000  0.091429  1.000000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Define paths, methods, and other configuration\n",
    "base_path = \"DS4NLP_results\"\n",
    "methods = [\"DeepEST\", \"GBS\", \"RHC-S\", \"SSRS\", \"SUPS\", 'SRS', '2-UPS']\n",
    "\n",
    "\n",
    "# Define mappings and true accuracies\n",
    "aux_var_mapping = {\n",
    "    \"confidence\": \"Confidence_Score\",\n",
    "    \"Confidence_Score\": \"Confidence_Score\",\n",
    "    \"entropy\": \"Prediction_Entropy\",\n",
    "    \"prediction\": \"Prediction_Entropy\",\n",
    "    \"Prediction_Entropy\": \"Prediction_Entropy\",\n",
    "    \"similarity\": \"Similarity_Score\",\n",
    "    \"Similarity_Score\": \"Similarity_Score\",\n",
    "    \"dsa\": \"DSA\",\n",
    "    \"DSA\": \"DSA\",\n",
    "    \"lsa\": \"LSA\",\n",
    "    \"LSA\": \"LSA\"\n",
    "}\n",
    "\n",
    "# True accuracies for each dataset\n",
    "true_accuracies = {\n",
    "    \"imdb300AuxDS\": 0.8990,\n",
    "    \"imdbAuxDS\": 0.8896,\n",
    "    \"SSTtestAuxDS\": 0.9226\n",
    "}\n",
    "\n",
    "def calculate_rmse_rmedse(accuracies, true_accuracy):\n",
    "    squared_errors = (accuracies - true_accuracy) ** 2\n",
    "    rmse = np.sqrt(np.mean(squared_errors))\n",
    "    rmedse = np.sqrt(np.median(squared_errors))\n",
    "    return rmse, rmedse\n",
    "\n",
    "results = []\n",
    "\n",
    "# Process each dataset for each method\n",
    "for method in methods:\n",
    "    for file_name in datasets.get(method, []):\n",
    "        dataset = file_name.split('_')[0]\n",
    "        true_accuracy = true_accuracies.get(dataset)\n",
    "        if not true_accuracy:\n",
    "            continue  # Skip if no true accuracy is defined\n",
    "\n",
    "        file_path = os.path.join(base_path, method, file_name)\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "            accuracies = data['accuracy'].apply(lambda x: float(str(x).split(\":\")[-1])).values\n",
    "            aux_var_key = file_name.split('_')[1].replace(\".csv\", \"\").lower()\n",
    "            aux_var = aux_var_mapping.get(aux_var_key, \"Unknown Variable\")\n",
    "            rmse, rmedse = calculate_rmse_rmedse(accuracies, true_accuracy)\n",
    "            results.append({\n",
    "                \"method\": method,\n",
    "                \"dataset\": dataset,\n",
    "                \"aux_var\": aux_var,\n",
    "                \"RMSE\": rmse,\n",
    "                \"RMedSE\": rmedse\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "if not results_df.empty:\n",
    "    pivot_df = results_df.pivot_table(index=['dataset', 'aux_var'], columns='method', values='RMSE', aggfunc=np.mean)\n",
    "\n",
    "    # Filter rows where all methods have data\n",
    "    pivot_df = pivot_df.dropna()\n",
    "\n",
    "    # Run Friedman test if there are enough data\n",
    "    if not pivot_df.empty and len(pivot_df.columns) >= 3:\n",
    "        try:\n",
    "            stat, p_value = friedmanchisquare(*[pivot_df[method].values for method in pivot_df.columns])\n",
    "            print(f'Friedman test statistic: {stat}, p-value: {p_value}')\n",
    "\n",
    "            if p_value < 0.05:\n",
    "                melted_df = pivot_df.reset_index().melt(id_vars=['dataset', 'aux_var'], var_name='method', value_name='RMSE')\n",
    "                dunn_results = sp.posthoc_dunn(melted_df, val_col='RMSE', group_col='method', p_adjust='bonferroni')\n",
    "                print(\"Dunn's posthoc test results:\")\n",
    "                print(dunn_results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error running Friedman test: {e}\")\n",
    "    else:\n",
    "        print(\"Not enough data to perform Friedman test.\")\n",
    "else:\n",
    "    print(\"No results to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c01a4d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset: imdb300AuxDS\n",
      "Friedman test for imdb300AuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "\n",
      "Results for dataset: imdbAuxDS\n",
      "Friedman test for imdbAuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "\n",
      "Results for dataset: SSTtestAuxDS\n",
      "Friedman test for SSTtestAuxDS: Statistic=6.0, p-value=0.42319008112684364\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Define paths and methods\n",
    "base_path = \"DS4NLP_results\"\n",
    "methods = [\"DeepEST\", \"GBS\", \"RHC-S\", \"SSRS\", \"SUPS\", 'SRS', '2-UPS']\n",
    "\n",
    "# Simulated data for illustration\n",
    "results = []\n",
    "for method in methods:\n",
    "    for dataset_category in [\"imdb300AuxDS\", \"imdbAuxDS\", \"SSTtestAuxDS\"]:\n",
    "        # Ensure multiple entries per method per dataset category to facilitate the Friedman test\n",
    "        results.extend([\n",
    "            {\"method\": method, \"dataset_category\": dataset_category, \"RMSE\": np.random.normal(0.1, 0.01)}\n",
    "            for _ in range(10)  # Generating multiple data points per method\n",
    "        ])\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Perform statistical tests for each major dataset category\n",
    "for dataset in [\"imdb300AuxDS\", \"imdbAuxDS\", \"SSTtestAuxDS\"]:\n",
    "    print(f\"\\nResults for dataset: {dataset}\")\n",
    "    dataset_df = results_df[results_df['dataset_category'] == dataset]\n",
    "    \n",
    "    if dataset_df['method'].nunique() >= 3:\n",
    "        pivot_df = dataset_df.pivot_table(index='method', values='RMSE', aggfunc=np.mean)\n",
    "\n",
    "        # The key is to ensure the pivot table is structured with one row per method\n",
    "        if pivot_df.shape[0] >= 3:  # Ensuring at least three methods are present\n",
    "            try:\n",
    "                # The Friedman test expects each array of values to compare across methods\n",
    "                stat, p_value = friedmanchisquare(*[pivot_df.loc[method].values for method in methods if method in pivot_df.index])\n",
    "                print(f\"Friedman test for {dataset}: Statistic={stat}, p-value={p_value}\")\n",
    "\n",
    "                if p_value < 0.05:\n",
    "                    dunn_results = sp.posthoc_dunn(dataset_df, val_col='RMSE', group_col='method', p_adjust='bonferroni')\n",
    "                    print(f\"Dunn's posthoc test results for {dataset}:\\n{dunn_results}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in statistical testing for {dataset}: {e}\")\n",
    "        else:\n",
    "            print(f\"Not enough methods with valid data for statistical testing for {dataset}.\")\n",
    "    else:\n",
    "        print(f\"Not enough methods with data for {dataset}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c9b701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for auxiliary variable: Confidence_Score\n",
      "Analyzing Confidence_Score in imdbAuxDS\n",
      "Friedman test for Confidence_Score in imdbAuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "Analyzing Confidence_Score in SSTIMDB3000AuxDS\n",
      "Missing data for some methods, skipping statistical tests.\n",
      "Analyzing Confidence_Score in SSTtestAuxDS\n",
      "Friedman test for Confidence_Score in SSTtestAuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "Analyzing Confidence_Score in imdb300AuxDS\n",
      "Friedman test for Confidence_Score in imdb300AuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "\n",
      "Results for auxiliary variable: Prediction_Entropy\n",
      "Analyzing Prediction_Entropy in imdbAuxDS\n",
      "Friedman test for Prediction_Entropy in imdbAuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "Analyzing Prediction_Entropy in imdb300AuxDS\n",
      "Friedman test for Prediction_Entropy in imdb300AuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "Analyzing Prediction_Entropy in SSTIMDB3000AuxDS\n",
      "Missing data for some methods, skipping statistical tests.\n",
      "Analyzing Prediction_Entropy in SSTtestAuxDS\n",
      "Friedman test for Prediction_Entropy in SSTtestAuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "\n",
      "Results for auxiliary variable: DSA\n",
      "Analyzing DSA in imdbAuxDS\n",
      "Friedman test for DSA in imdbAuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "Analyzing DSA in imdb300AuxDS\n",
      "Friedman test for DSA in imdb300AuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "Analyzing DSA in SSTIMDB3000AuxDS\n",
      "Missing data for some methods, skipping statistical tests.\n",
      "Analyzing DSA in SSTtestAuxDS\n",
      "Friedman test for DSA in SSTtestAuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "\n",
      "Results for auxiliary variable: LSA\n",
      "Analyzing LSA in imdbAuxDS\n",
      "Friedman test for LSA in imdbAuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "Analyzing LSA in imdb300AuxDS\n",
      "Friedman test for LSA in imdb300AuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "Analyzing LSA in SSTIMDB3000AuxDS\n",
      "Missing data for some methods, skipping statistical tests.\n",
      "Analyzing LSA in SSTtestAuxDS\n",
      "Friedman test for LSA in SSTtestAuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "\n",
      "Results for auxiliary variable: Similarity_Score\n",
      "Analyzing Similarity_Score in imdbAuxDS\n",
      "Friedman test for Similarity_Score in imdbAuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "Analyzing Similarity_Score in SSTIMDB3000AuxDS\n",
      "Missing data for some methods, skipping statistical tests.\n",
      "Analyzing Similarity_Score in SSTtestAuxDS\n",
      "Friedman test for Similarity_Score in SSTtestAuxDS: Statistic=6.0, p-value=0.42319008112684364\n",
      "Analyzing Similarity_Score in imdb300AuxDS\n",
      "Friedman test for Similarity_Score in imdb300AuxDS: Statistic=6.0, p-value=0.42319008112684364\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Define paths and methods\n",
    "base_path = \"DS4NLP_results\"\n",
    "methods = [\"DeepEST\", \"GBS\", \"RHC-S\", \"SSRS\", \"SUPS\", 'SRS', '2-UPS']\n",
    "\n",
    "# Auxiliary variable mapping\n",
    "aux_var_mapping = {\n",
    "    \"confidence\": \"Confidence_Score\",\n",
    "    \"Confidence_Score\": \"Confidence_Score\",\n",
    "    \"entropy\": \"Prediction_Entropy\",\n",
    "    \"prediction\": \"Prediction_Entropy\",\n",
    "    \"Prediction_Entropy\": \"Prediction_Entropy\",\n",
    "    \"similarity\": \"Similarity_Score\",\n",
    "    \"Similarity_Score\": \"Similarity_Score\",\n",
    "    \"dsa\": \"DSA\",\n",
    "    \"DSA\": \"DSA\",\n",
    "    \"lsa\": \"LSA\",\n",
    "    \"LSA\": \"LSA\"\n",
    "}\n",
    "\n",
    "# Simulated loading of data\n",
    "results = []\n",
    "for method in methods:\n",
    "    method_path = os.path.join(base_path, method)\n",
    "    for filename in os.listdir(method_path):\n",
    "        parts = filename.replace('.csv', '').split('_')\n",
    "        if len(parts) >= 2:\n",
    "            dataset_category = parts[0]\n",
    "            aux_var_key = parts[1].lower()\n",
    "            aux_var = aux_var_mapping.get(aux_var_key, \"Unknown Variable\")\n",
    "            if aux_var != \"Unknown Variable\":\n",
    "                # Simulate more realistic RMSE calculations\n",
    "                simulated_rmse = np.random.normal(0.05, 0.01) + np.random.rand() * 0.01\n",
    "                results.append({\n",
    "                    \"method\": method,\n",
    "                    \"dataset_category\": dataset_category,\n",
    "                    \"aux_var\": aux_var,\n",
    "                    \"RMSE\": simulated_rmse\n",
    "                })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Perform statistical tests for each auxiliary variable across all datasets\n",
    "for aux in set(results_df['aux_var']):\n",
    "    if aux == \"Unknown Variable\":\n",
    "        continue\n",
    "    print(f\"\\nResults for auxiliary variable: {aux}\")\n",
    "    aux_df = results_df[results_df['aux_var'] == aux]\n",
    "    \n",
    "    for dataset in set(aux_df['dataset_category']):\n",
    "        print(f\"Analyzing {aux} in {dataset}\")\n",
    "        dataset_df = aux_df[aux_df['dataset_category'] == dataset]\n",
    "\n",
    "        if dataset_df['method'].nunique() >= 3:\n",
    "            pivot_df = dataset_df.pivot_table(index='method', values='RMSE', aggfunc='mean')\n",
    "\n",
    "            if not all(m in pivot_df.index for m in methods):\n",
    "                print(\"Missing data for some methods, skipping statistical tests.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                stats = friedmanchisquare(*(pivot_df.loc[m].values for m in pivot_df.index))\n",
    "                stat, p_value = stats\n",
    "                print(f\"Friedman test for {aux} in {dataset}: Statistic={stat}, p-value={p_value}\")\n",
    "\n",
    "                if p_value < 0.05:\n",
    "                    dunn_results = sp.posthoc_dunn(dataset_df, val_col='RMSE', group_col='method', p_adjust='bonferroni')\n",
    "                    print(f\"Dunn's posthoc test results for {aux} in {dataset}:\\n{dunn_results}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in statistical testing for {aux} in {dataset}: {e}\")\n",
    "        else:\n",
    "            print(f\"Not enough methods with data for {aux} in {dataset}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ee313",
   "metadata": {},
   "source": [
    "## RQ3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "48afb9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for budget 50:\n",
      "      method       dataset             aux_var      RMSE    RMedSE  \\\n",
      "0    DeepEST  imdb300AuxDS    Confidence_Score  0.082051  0.077985   \n",
      "1    DeepEST  imdb300AuxDS                 DSA  0.048502  0.039416   \n",
      "2    DeepEST  imdb300AuxDS  Prediction_Entropy  0.089145  0.068531   \n",
      "3    DeepEST  imdb300AuxDS                 LSA  0.010321  0.004452   \n",
      "4    DeepEST  imdb300AuxDS    Similarity_Score  0.103185  0.098213   \n",
      "..       ...           ...                 ...       ...       ...   \n",
      "100    2-UPS  SSTtestAuxDS    Confidence_Score  0.042546  0.033479   \n",
      "101    2-UPS  SSTtestAuxDS                 DSA  0.088936  0.052977   \n",
      "102    2-UPS  SSTtestAuxDS                 LSA  0.074649  0.077430   \n",
      "103    2-UPS  SSTtestAuxDS  Prediction_Entropy  0.061844  0.057430   \n",
      "104    2-UPS  SSTtestAuxDS    Similarity_Score  0.043404  0.020164   \n",
      "\n",
      "     failures_mean  failures_std  \n",
      "0        17.166667      2.742807  \n",
      "1         5.300000      2.451600  \n",
      "2        18.433333      2.955805  \n",
      "3        41.066667      2.851900  \n",
      "4         1.266667      1.014833  \n",
      "..             ...           ...  \n",
      "100       3.900000      2.040115  \n",
      "101       6.366667      3.388452  \n",
      "102       0.166667      0.461133  \n",
      "103       0.933333      0.980265  \n",
      "104       4.233333      2.176415  \n",
      "\n",
      "[105 rows x 7 columns]\n",
      "Results for budget 100:\n",
      "      method       dataset             aux_var      RMSE    RMedSE  \\\n",
      "0    DeepEST  imdb300AuxDS    Confidence_Score  0.051546  0.030861   \n",
      "1    DeepEST  imdb300AuxDS                 DSA  0.030858  0.020508   \n",
      "2    DeepEST  imdb300AuxDS  Prediction_Entropy  0.055034  0.028153   \n",
      "3    DeepEST  imdb300AuxDS                 LSA  0.009967  0.003403   \n",
      "4    DeepEST  imdb300AuxDS    Similarity_Score  0.057575  0.051080   \n",
      "..       ...           ...                 ...       ...       ...   \n",
      "100    2-UPS  SSTtestAuxDS    Confidence_Score  0.038173  0.024121   \n",
      "101    2-UPS  SSTtestAuxDS                 DSA  0.066572  0.042307   \n",
      "102    2-UPS  SSTtestAuxDS                 LSA  0.073071  0.077430   \n",
      "103    2-UPS  SSTtestAuxDS  Prediction_Entropy  0.062857  0.067430   \n",
      "104    2-UPS  SSTtestAuxDS    Similarity_Score  0.026292  0.022570   \n",
      "\n",
      "     failures_mean  failures_std  \n",
      "0        35.666667      3.526582  \n",
      "1        10.066667      3.139661  \n",
      "2        36.700000      3.896506  \n",
      "3        80.833333      2.937080  \n",
      "4         2.000000      1.259447  \n",
      "..             ...           ...  \n",
      "100       8.800000      3.566124  \n",
      "101      11.333333      5.435092  \n",
      "102       0.466667      0.681445  \n",
      "103       1.600000      1.354431  \n",
      "104       7.766667      2.674056  \n",
      "\n",
      "[105 rows x 7 columns]\n",
      "Results for budget 200:\n",
      "      method       dataset             aux_var      RMSE    RMedSE  \\\n",
      "0    DeepEST  imdb300AuxDS    Confidence_Score  0.046567  0.026221   \n",
      "1    DeepEST  imdb300AuxDS                 DSA  0.016135  0.008057   \n",
      "2    DeepEST  imdb300AuxDS  Prediction_Entropy  0.041714  0.026660   \n",
      "3    DeepEST  imdb300AuxDS                 LSA  0.007369  0.002580   \n",
      "4    DeepEST  imdb300AuxDS    Similarity_Score  0.033537  0.019886   \n",
      "..       ...           ...                 ...       ...       ...   \n",
      "100    2-UPS  SSTtestAuxDS    Confidence_Score  0.017719  0.012720   \n",
      "101    2-UPS  SSTtestAuxDS                 DSA  0.058226  0.052578   \n",
      "102    2-UPS  SSTtestAuxDS                 LSA  0.072877  0.072430   \n",
      "103    2-UPS  SSTtestAuxDS  Prediction_Entropy  0.060796  0.059982   \n",
      "104    2-UPS  SSTtestAuxDS    Similarity_Score  0.016463  0.007570   \n",
      "\n",
      "     failures_mean  failures_std  \n",
      "0        72.233333      2.967448  \n",
      "1        20.300000      3.281610  \n",
      "2        73.300000      3.621297  \n",
      "3       161.533333      5.975429  \n",
      "4         4.333333      1.422318  \n",
      "..             ...           ...  \n",
      "100      16.566667      3.297683  \n",
      "101      24.600000      6.703576  \n",
      "102       0.933333      0.827682  \n",
      "103       3.466667      1.870521  \n",
      "104      15.200000      3.336321  \n",
      "\n",
      "[105 rows x 7 columns]\n",
      "Results for budget 400:\n",
      "      method       dataset             aux_var      RMSE    RMedSE  \\\n",
      "0    DeepEST  imdb300AuxDS    Confidence_Score  0.019422  0.014388   \n",
      "1    DeepEST  imdb300AuxDS                 DSA  0.015008  0.012134   \n",
      "2    DeepEST  imdb300AuxDS  Prediction_Entropy  0.025431  0.019706   \n",
      "3    DeepEST  imdb300AuxDS                 LSA  0.006272  0.003139   \n",
      "4    DeepEST  imdb300AuxDS    Similarity_Score  0.032188  0.017758   \n",
      "..       ...           ...                 ...       ...       ...   \n",
      "100    2-UPS  SSTtestAuxDS    Confidence_Score  0.016710  0.010432   \n",
      "101    2-UPS  SSTtestAuxDS                 DSA  0.057516  0.047812   \n",
      "102    2-UPS  SSTtestAuxDS                 LSA  0.072018  0.072430   \n",
      "103    2-UPS  SSTtestAuxDS  Prediction_Entropy  0.059935  0.059930   \n",
      "104    2-UPS  SSTtestAuxDS    Similarity_Score  0.011467  0.007500   \n",
      "\n",
      "     failures_mean  failures_std  \n",
      "0        88.500000      4.183300  \n",
      "1        39.733333      6.039715  \n",
      "2       105.700000      3.505169  \n",
      "3       296.366667      0.614948  \n",
      "4         8.966667      2.735253  \n",
      "..             ...           ...  \n",
      "100      33.266667      6.241868  \n",
      "101      49.933333     12.533908  \n",
      "102       2.200000      1.447947  \n",
      "103       7.100000      2.249138  \n",
      "104      30.466667      4.636685  \n",
      "\n",
      "[105 rows x 7 columns]\n",
      "Results for budget 800:\n",
      "      method       dataset             aux_var      RMSE    RMedSE  \\\n",
      "0    DeepEST  imdb300AuxDS    Confidence_Score  0.012305  0.009177   \n",
      "1    DeepEST  imdb300AuxDS                 DSA  0.008915  0.006724   \n",
      "2    DeepEST  imdb300AuxDS  Prediction_Entropy  0.014389  0.012966   \n",
      "3    DeepEST  imdb300AuxDS                 LSA  0.002814  0.001647   \n",
      "4    DeepEST  imdb300AuxDS    Similarity_Score  0.020008  0.013266   \n",
      "..       ...           ...                 ...       ...       ...   \n",
      "100    2-UPS  SSTtestAuxDS    Confidence_Score  0.013013  0.007112   \n",
      "101    2-UPS  SSTtestAuxDS                 DSA  0.047328  0.044303   \n",
      "102    2-UPS  SSTtestAuxDS                 LSA  0.071564  0.071808   \n",
      "103    2-UPS  SSTtestAuxDS  Prediction_Entropy  0.057237  0.056808   \n",
      "104    2-UPS  SSTtestAuxDS    Similarity_Score  0.010627  0.005729   \n",
      "\n",
      "     failures_mean  failures_std  \n",
      "0       123.266667      8.055554  \n",
      "1        81.366667      7.107467  \n",
      "2       135.000000      6.125132  \n",
      "3       297.433333      1.040004  \n",
      "4        18.633333      3.586212  \n",
      "..             ...           ...  \n",
      "100      68.333333      7.866706  \n",
      "101      97.033333     13.435628  \n",
      "102       4.733333      2.180372  \n",
      "103      16.366667      4.475861  \n",
      "104      62.066667      8.646041  \n",
      "\n",
      "[105 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the base path, methods, and budget levels\n",
    "base_path = \"DS4NLP_results\"\n",
    "methods = [\"DeepEST\", \"GBS\", \"RHC-S\", \"SSRS\", \"SUPS\", 'SRS', '2-UPS']\n",
    "budgets = [50, 100, 200, 400, 800]\n",
    "\n",
    "# True accuracies for each dataset\n",
    "true_accuracies = {\n",
    "    \"imdb300AuxDS\": 0.8990,\n",
    "    \"imdbAuxDS\": 0.8896,\n",
    "    \"SSTtestAuxDS\": 0.9225700164744646\n",
    "}\n",
    "\n",
    "# Mapping of aux variable names to standard names\n",
    "aux_var_mapping = {\n",
    "    \"confidence\": \"Confidence_Score\",\n",
    "    \"Confidence_Score\": \"Confidence_Score\",\n",
    "    \"entropy\": \"Prediction_Entropy\",\n",
    "    \"prediction\": \"Prediction_Entropy\",\n",
    "    \"Prediction_Entropy\": \"Prediction_Entropy\",\n",
    "    \"similarity\": \"Similarity_Score\",\n",
    "    \"Similarity_Score\": \"Similarity_Score\",\n",
    "    \"dsa\": \"DSA\",\n",
    "    \"DSA\": \"DSA\",\n",
    "    \"lsa\": \"LSA\",\n",
    "    \"LSA\": \"LSA\"\n",
    "}\n",
    "\n",
    "# Function to calculate RMSE and RMedSE\n",
    "def calculate_rmse_rmedse(accuracies, true_accuracy):\n",
    "    squared_errors = (accuracies - true_accuracy) ** 2\n",
    "    rmse = np.sqrt(np.mean(squared_errors))\n",
    "    rmedse = np.sqrt(np.median(squared_errors))\n",
    "    return rmse, rmedse\n",
    "\n",
    "# Iterate over each budget\n",
    "for budget in budgets:\n",
    "    results = []\n",
    "\n",
    "    # Process each method and dataset\n",
    "    for method in methods:\n",
    "        for file_name in datasets[method]:\n",
    "            dataset = file_name.split('_')[0]\n",
    "\n",
    "            if dataset not in true_accuracies:\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(base_path, method, file_name)\n",
    "\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                data = pd.read_csv(file_path)\n",
    "                data = data[data['budget'] == budget]\n",
    "                if data.empty:\n",
    "                    print(f\"No records with budget {budget} in {file_name}\")\n",
    "                    continue\n",
    "\n",
    "                accuracies = data['accuracy'].values\n",
    "                rmse, rmedse = calculate_rmse_rmedse(accuracies, true_accuracies[dataset])\n",
    "                failures_mean = data['failures'].mean()\n",
    "                failures_std = data['failures'].std()\n",
    "\n",
    "                results.append({\n",
    "                    \"method\": method,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"aux_var\": aux_var_mapping.get(file_name.split('_')[1].split('.')[0].lower(), \"Unknown Variable\"),\n",
    "                    \"RMSE\": rmse,\n",
    "                    \"RMedSE\": rmedse,\n",
    "                    \"failures_mean\": failures_mean,\n",
    "                    \"failures_std\": failures_std\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    # Convert results to DataFrame and display\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"Results for budget {budget}:\")\n",
    "    print(results_df)\n",
    "\n",
    "    # Save the results to a CSV file for each budget\n",
    "    results_df.to_csv(f\"rmse_rmedse_failures{budget}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "517fb4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    method       dataset             aux_var      RMSE    RMedSE  \\\n",
      "0  DeepEST  imdb300AuxDS    Confidence_Score  0.082051  0.077985   \n",
      "1  DeepEST  imdb300AuxDS                 DSA  0.048502  0.039416   \n",
      "2  DeepEST  imdb300AuxDS  Prediction_Entropy  0.089145  0.068531   \n",
      "3  DeepEST  imdb300AuxDS                 LSA  0.010321  0.004452   \n",
      "4  DeepEST  imdb300AuxDS    Similarity_Score  0.103185  0.098213   \n",
      "\n",
      "   failures_mean  failures_std  budget  \n",
      "0      17.166667      2.742807      50  \n",
      "1       5.300000      2.451600      50  \n",
      "2      18.433333      2.955805      50  \n",
      "3      41.066667      2.851900      50  \n",
      "4       1.266667      1.014833      50  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the base path and budgets\n",
    "base_path = \"./\"  # Update this to the location of your datasets\n",
    "budgets = [50, 100, 200, 400, 800]\n",
    "file_names = [f\"rmse_rmedse_failures{budget}.csv\" for budget in budgets]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Load each dataset and add the 'budget' column\n",
    "for file_name, budget in zip(file_names, budgets):\n",
    "    file_path = f\"{base_path}/{file_name}\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['budget'] = budget\n",
    "    all_data.append(df)\n",
    "\n",
    "# Concatenate all datasets into a single DataFrame\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Now you can work with the combined DataFrame\n",
    "print(combined_df.head())  # To check the top rows of the combined DataFrame\n",
    "# Optionally save the combined DataFrame to a CSV file\n",
    "combined_df.to_csv(f\"{base_path}/combined_datasetsRQ3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640caa4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
